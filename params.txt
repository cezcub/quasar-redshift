neural_network.MLPRegressor(hidden_layer_sizes=(200, 100, 50, 25),learning_rate_init=0.001,max_iter=800,alpha=0.01)
neighbors.KNeighborsRegressor(n_neighbors=10, weights='distance', metric='manhattan')
ensemble.RandomForestRegressor(max_depth=50, min_samples_split=2, min_samples_leaf=2, n_estimators=500, bootstrap=True)
xgb.XGBRegressor(n_estimators=800, max_depth=10, learning_rate=0.015, subsample=0.82, reg_alpha=0.02, reg_lambda=0.04, colsample_bytree=1)
lgb.LGBMRegressor('n_estimators': 800, 'max_depth': -1, 'learning_rate': 0.022, 'feature_fraction': 0.98, 'bagging_fraction': 0.77, 'bagging_freq': 1, 'min_child_samples': 5, 'reg_alpha': 0.0015, 'reg_lambda': 0.27, 'num_leaves': 255)
CatBoostRegressor(iterations=800, depth=8, learning_rate=0.78, subsample=0.74, colsample_bylevel=0.85, min_data_in_leaf=30, reg_lambda=0.0015, random_strength=0.59, bagging_temperature=0.2, border_count=128)
QuasarCNN(conv_channels=[32, 128, 64], fc_sizes=[512, 128, 128], conv_dropout=0.18, fc_dropout=0.26, learning_rate=0.0025, batch_size=64, weight_decay=3e-06)
QuasarTransformer(d_model=512, nhead=8, num_layers=4, dim_feedforward=1024, dropout=0.050, activation='gelu', output_layers=[512, 512, 64], learning_rate=2.76e-05, batch_size=32, weight_decay=7.08e-06, grad_clip_norm=0.584)
QuasarEfficientNet(model_size='b4', dropout_rate=0.332, drop_connect_rate=0.196, learning_rate=0.00973, batch_size=64, weight_decay=0.00125, grad_clip_norm=0.584, optimizer_type='adamw')
QuasarConvNeXt(model_size='tiny', dropout_rate=0.152, drop_path_rate=0.232, layer_scale_init_value=1.68e-07, learning_rate=0.000161, batch_size=32, weight_decay=0.00841, grad_clip_norm=1.702, optimizer_type='adamw', use_attention=False)